# Speech-to-Text using Hidden Markov Models with Gaussian Emissions (GMM-HMM)

[![License](https://img.shields.io/badge/License-MIT-blue.svg)](LICENSE)
[![Python](https://img.shields.io/badge/Python-3.7+-green.svg)](https://www.python.org/)
[![Kaldi](https://img.shields.io/badge/Kaldi-ASR-orange.svg)](https://kaldi-asr.org/)

An Automatic Speech Recognition (ASR) system implementing Gaussian Mixture Model-Hidden Markov Model (GMM-HMM) architecture for audio transcription, developed as part of the Probabilistic Reasoning course (22AIE301).

## ğŸ“‹ Table of Contents

- [Overview](#overview)
- [Features](#features)
- [System Architecture](#system-architecture)
- [Dataset](#dataset)
- [Installation](#installation)
- [Usage](#usage)
- [Model Training Pipeline](#model-training-pipeline)
- [Results](#results)
- [Course Mapping](#course-mapping)
- [Contributors](#contributors)
- [References](#references)

## ğŸ¯ Overview

This project implements a complete ASR pipeline using GMM-HMM models for Tamil language speech recognition. The system extracts Mel-Frequency Cepstral Coefficients (MFCC) features from audio, models temporal patterns with Hidden Markov Models, and uses Gaussian Mixture Models for acoustic feature representation.

### Key Objectives

- Build an accurate audio-to-text transcription system using GMM-HMM
- Extract and utilize MFCC features for speech representation (39-dimensional)
- Model temporal speech patterns with HMM and acoustic features with GMM
- Implement Baum-Welch algorithm for parameter estimation
- Apply Viterbi decoding for transcription

## âœ¨ Features

- **Multi-stage Acoustic Modeling**: Progressive training from monophone to triphone models
- **Feature Extraction**: 39-D MFCC features with delta and delta-delta coefficients
- **Advanced Transformations**: LDA+MLLT for feature optimization
- **Language Model Integration**: Trigram language model with Witten-Bell smoothing
- **Kaldi Framework**: Industry-standard ASR toolkit implementation
- **Low-resource Language Support**: Optimized for Tamil language recognition

## ğŸ—ï¸ System Architecture

### Complete ASR Pipeline Architecture

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                    SPEECH-TO-TEXT GMM-HMM ASR SYSTEM                         â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ INPUT STAGE                                                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                              â”‚
â”‚  Raw Audio (MP3, 48kHz) â”€â”€â”€â”€â”€â”€â–º Audio Preprocessing â”€â”€â”€â”€â”€â”€â–º WAV (16kHz, mono)
â”‚                                  â€¢ Format conversion                         â”‚
â”‚                                  â€¢ Resampling                                â”‚
â”‚                                  â€¢ Normalization                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                         â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ FEATURE EXTRACTION STAGE                                                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                              â”‚
â”‚  Windowing (25ms frames, 10ms shift)                                         â”‚
â”‚         â”‚                                                                    â”‚
â”‚         â”œâ”€â–º FFT â”€â–º Mel Filterbank â”€â–º Log â”€â–º DCT â”€â–º MFCC (13 coefficients)  â”‚
â”‚         â”‚                                                                    â”‚
â”‚         â”œâ”€â–º Compute Î” (Delta) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º 13 coefficients          â”‚
â”‚         â”‚                                                                    â”‚
â”‚         â””â”€â–º Compute Î”Î” (Delta-Delta) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º 13 coefficients          â”‚
â”‚                                                                              â”‚
â”‚  Combined Feature Vector: 39 dimensions (13 + 13Î” + 13Î”Î”)                   â”‚
â”‚         â”‚                                                                    â”‚
â”‚         â””â”€â–º CMVN Normalization (mean=0, variance=1)                          â”‚
â”‚                                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                         â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ACOUSTIC MODEL TRAINING (Baum-Welch EM Algorithm)                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚ Stage 1: MONOPHONE MODEL (Context-Independent)                  â”‚        â”‚
â”‚  â”‚  â€¢ States: 112 PDFs                                             â”‚        â”‚
â”‚  â”‚  â€¢ Gaussians: 986                                               â”‚        â”‚
â”‚  â”‚  â€¢ Context: Single phone                                        â”‚        â”‚
â”‚  â”‚  â€¢ WER: 8.0% | Training: ~30 min                                â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚                       â”‚ Alignment                                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚ Stage 2: TRIPHONE MODEL (Context-Dependent)                     â”‚        â”‚
â”‚  â”‚  â€¢ States: 456 PDFs                                             â”‚        â”‚
â”‚  â”‚  â€¢ Gaussians: 10,039 (~22/state)                                â”‚        â”‚
â”‚  â”‚  â€¢ Context: 3 phones (L-C-R)                                    â”‚        â”‚
â”‚  â”‚  â€¢ WER: 3.2% | Training: ~2 hours                               â”‚        â”‚
â”‚  â”‚  â€¢ Improvement: 60% WER reduction âœ“                             â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚                       â”‚ Alignment                                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚ Stage 3: LDA+MLLT MODEL (Feature Transform)                     â”‚        â”‚
â”‚  â”‚  â€¢ States: 528 PDFs                                             â”‚        â”‚
â”‚  â”‚  â€¢ Gaussians: 13,867 (~26/state)                                â”‚        â”‚
â”‚  â”‚  â€¢ Features: 40-D (LDA transformation)                          â”‚        â”‚
â”‚  â”‚  â€¢ WER: 3.0% | Training: ~3 hours                               â”‚        â”‚
â”‚  â”‚  â€¢ Improvement: 62.5% total WER reduction âœ“âœ“                    â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚                                                                              â”‚
â”‚  GMM Parameters: {Î¼â‚–, Î£â‚–, wâ‚–} learned via EM                                â”‚
â”‚  HMM Parameters: {Ï€, A, B} learned via Baum-Welch                            â”‚
â”‚                                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                         â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ LANGUAGE MODEL & LEXICON                                                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                        â”‚
â”‚  â”‚  LEXICON (L.fst)    â”‚     â”‚  LANGUAGE MODEL     â”‚                        â”‚
â”‚  â”‚                     â”‚     â”‚  (G.fst)            â”‚                        â”‚
â”‚  â”‚  Words â†’ Phonemes   â”‚     â”‚                     â”‚                        â”‚
â”‚  â”‚  à®…à®ƒà®¤à®±à®®à¯ â†’ /a/ /á¸µ/   â”‚     â”‚  Trigram Model      â”‚                        â”‚
â”‚  â”‚  /t/ /a/ /á¹Ÿ/ /a/    â”‚     â”‚  P(wâ‚ƒ|wâ‚,wâ‚‚)        â”‚                        â”‚
â”‚  â”‚  /m/                â”‚     â”‚                     â”‚                        â”‚
â”‚  â”‚                     â”‚     â”‚  Witten-Bell        â”‚                        â”‚
â”‚  â”‚  12,000+ entries    â”‚     â”‚  Smoothing          â”‚                        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                        â”‚
â”‚                                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                         â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ DECODING STAGE (Viterbi Algorithm)                                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                              â”‚
â”‚  Weighted Finite-State Transducer (WFST) Composition:                       â”‚
â”‚                                                                              â”‚
â”‚         H âˆ˜ C âˆ˜ L âˆ˜ G = HCLG.fst                                             â”‚
â”‚         â”‚   â”‚   â”‚   â”‚                                                        â”‚
â”‚         â”‚   â”‚   â”‚   â””â”€â–º Grammar (Language Model)                             â”‚
â”‚         â”‚   â”‚   â””â”€â”€â”€â”€â”€â–º Lexicon (Pronunciation Dictionary)                   â”‚
â”‚         â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º Context-dependency (Triphone contexts)               â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º HMM structure (Acoustic topology)                    â”‚
â”‚                                                                              â”‚
â”‚  Viterbi Beam Search:                                                        â”‚
â”‚    â€¢ Finds most likely word sequence W* = argmax P(W|O)                     â”‚
â”‚    â€¢ Combines: Acoustic score + Language score + Pronunciation score        â”‚
â”‚    â€¢ Beam pruning for efficiency                                            â”‚
â”‚                                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                         â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ OUTPUT STAGE                                                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                              â”‚
â”‚  Transcribed Text Output                                                     â”‚
â”‚  â€¢ Word sequence with confidence scores                                      â”‚
â”‚  â€¢ Timestamp alignments (optional)                                           â”‚
â”‚  â€¢ WER: 3.0% (97% accuracy) âœ“âœ“âœ“                                             â”‚
â”‚                                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Mathematical Framework

#### GMM-HMM Joint Probability Model

```
P(O, Q | Î») = P(Q | Î») Â· P(O | Q, Î»)
            = Ï€(qâ‚) Â· âˆáµ—â‚Œâ‚‚áµ€ a(qâ‚œâ‚‹â‚, qâ‚œ) Â· âˆáµ—â‚Œâ‚áµ€ bqâ‚œ(oâ‚œ)

Where:
  O = {oâ‚, oâ‚‚, ..., oâ‚œ}  : Observation sequence (MFCC features)
  Q = {qâ‚, qâ‚‚, ..., qâ‚œ}  : Hidden state sequence (phonemes)
  Î» = {Ï€, A, B}          : HMM parameters
  
  Ï€(qáµ¢)    : Initial state probability
  a(i,j)   : Transition probability P(qâ‚œ=j | qâ‚œâ‚‹â‚=i)
  bâ±¼(oâ‚œ)   : Observation probability P(oâ‚œ | qâ‚œ=j) â† GMM
```

#### GMM Observation Probability

```
P(oâ‚œ | qâ‚œ=j) = âˆ‘â‚–â‚Œâ‚á´· wâ±¼â‚– Â· ğ’©(oâ‚œ | Î¼â±¼â‚–, Î£â±¼â‚–)

Where:
  K      : Number of Gaussian components
  wâ±¼â‚–    : Mixture weight for component k in state j
  Î¼â±¼â‚–    : Mean vector
  Î£â±¼â‚–    : Covariance matrix
  ğ’©()    : Multivariate Gaussian distribution
```

### Component Breakdown

| Component | Function | Input | Output |
|-----------|----------|-------|--------|
| **GMM** | Models observation likelihoods | MFCC features | P(oâ‚œ \| qâ‚œ) |
| **HMM** | Models temporal structure | State sequence | P(qâ‚œ \| qâ‚œâ‚‹â‚) |
| **MFCC** | Feature extraction | Audio waveform | 39-D vectors |
| **LDA+MLLT** | Feature transformation | 39-D features | 40-D features |
| **Lexicon** | Word-to-phoneme mapping | Words | Phoneme sequences |
| **Language Model** | Context prediction | Word history | P(wâ‚œ \| wâ‚œâ‚‹â‚‚, wâ‚œâ‚‹â‚) |
| **Viterbi Decoder** | Best path search | Features + Models | Text output |

### Data Flow Summary

```
Audio (48kHz) â†’ Preprocessing â†’ WAV (16kHz) â†’ MFCC Extraction â†’ 39-D Features
â†’ CMVN Normalization â†’ GMM-HMM Training (3 stages) â†’ Acoustic Models
â†’ WFST Composition (Hâˆ˜Câˆ˜Lâˆ˜G) â†’ Viterbi Decoding â†’ Transcribed Text (97% accuracy)
```

## ğŸ“Š Dataset

**Common Voice Corpus 6.1 - Tamil**

| Parameter | Value |
|-----------|-------|
| Version | cv-corpus-6.1-2020-12-11 |
| Release Date | December 11, 2020 |
| Language | Tamil (ta) |
| Total Duration | 24 hours |
| Validated Duration | 14 hours (58.3%) |
| Format | MP3 (48kHz) â†’ WAV (16kHz mono) |
| Bit Depth | 16-bit |
| Training Clips | 12,652 utterances |
| Test Clips | 1,781 utterances |
| Total Validated | 14,433+ clips |
| Speakers | Crowdsourced (diverse) |
| Download Size | ~1-2 GB (tar.gz) |

**Download**: [Mozilla Common Voice](https://commonvoice.mozilla.org/en/datasets)

## ğŸ”§ Installation

### Prerequisites

- **Operating System**: Ubuntu 20.04 (or WSL2 on Windows)
- **RAM**: Minimum 8GB (16GB recommended)
- **Storage**: 10GB free space
- **Compiler**: GCC 7.5+

### Step 1: Install System Dependencies

```bash
# Update package list
sudo apt-get update

# Build tools
sudo apt-get install -y gcc g++ make autoconf automake libtool \
    cmake git wget unzip sox

# Math libraries
sudo apt-get install -y libatlas-base-dev liblapack-dev libblas-dev

# Audio processing
sudo apt-get install -y sox ffmpeg libsndfile1-dev

# Python dependencies
sudo apt-get install -y python3 python3-pip
pip3 install numpy scipy matplotlib
```

### Step 2: Install Kaldi

```bash
# Clone Kaldi repository
git clone https://github.com/kaldi-asr/kaldi.git
cd kaldi

# Build tools
cd tools
make -j $(nproc)

# Install SRILM (for language modeling)
# Download from: http://www.speech.sri.com/projects/srilm/download.html
# Extract and compile according to SRILM documentation

# Build Kaldi source
cd ../src
./configure --shared
make depend -j $(nproc)
make -j $(nproc)
```

### Step 3: Setup Project

```bash
# Clone this repository
git clone https://github.com/yourusername/gmm-hmm-asr.git
cd gmm-hmm-asr

# Create data directories
mkdir -p data/train data/test data/local/dict data/local/lang

# Convert audio files to required format
for file in raw_audio/*.mp3; do
    sox "$file" -r 16000 -c 1 -b 16 "data/wav/$(basename $file .mp3).wav"
done
```

## ğŸš€ Usage

### Data Preparation

```bash
# Prepare data files
python scripts/prepare_data.py \
    --corpus_dir /path/to/common_voice \
    --output_dir data/

# Prepare lexicon
python scripts/prepare_dict.py \
    --train_text data/train/text \
    --dict_dir data/local/dict
```

### Feature Extraction

```bash
# Extract MFCC features
steps/make_mfcc.sh --nj 4 --cmd "run.pl" \
    data/train exp/make_mfcc/train mfcc

steps/make_mfcc.sh --nj 4 --cmd "run.pl" \
    data/test exp/make_mfcc/test mfcc

# Compute CMVN statistics
steps/compute_cmvn_stats.sh \
    data/train exp/make_mfcc/train mfcc
```

### Model Training

```bash
# 1. Train Monophone Model
steps/train_mono.sh --nj 4 --cmd "run.pl" \
    data/train data/lang exp/mono

# 2. Align with Monophone
steps/align_si.sh --nj 4 --cmd "run.pl" \
    data/train data/lang exp/mono exp/mono_ali

# 3. Train Triphone Model
steps/train_deltas.sh --cmd "run.pl" 2000 10000 \
    data/train data/lang exp/mono_ali exp/tri1

# 4. Align with Triphone
steps/align_si.sh --nj 4 --cmd "run.pl" \
    data/train data/lang exp/tri1 exp/tri1_ali

# 5. Train LDA+MLLT Model
steps/train_lda_mllt.sh --cmd "run.pl" 2500 15000 \
    data/train data/lang exp/tri1_ali exp/tri2b
```

### Language Model Creation

```bash
# Build trigram language model
python scripts/prepare_lm.py \
    --text data/train/text \
    --output data/local/lm/trigram.arpa

# Convert to FST format
utils/format_lm.sh data/lang data/local/lm/trigram.arpa \
    data/local/dict/lexicon.txt data/lang_test
```

### Decoding

```bash
# Create decoding graph
utils/mkgraph.sh data/lang_test exp/tri2b exp/tri2b/graph

# Decode test set
steps/decode.sh --nj 4 --cmd "run.pl" \
    exp/tri2b/graph data/test exp/tri2b/decode_test

# Calculate Word Error Rate (WER)
grep WER exp/tri2b/decode_test/wer_* | utils/best_wer.sh
```

### Transcribe New Audio

```bash
# Transcribe a single audio file
python scripts/transcribe.py \
    --audio input.wav \
    --model exp/tri2b \
    --graph exp/tri2b/graph \
    --output transcription.txt
```

## ğŸ”¬ Model Training Pipeline

### Acoustic Models

#### 1. Monophone Model (mono)
- **Type**: Context-independent (flat-start)
- **States**: 112 PDFs
- **Gaussians**: 986
- **Context Width**: 1
- **Training Iterations**: 40
- **Purpose**: Baseline model for alignment

#### 2. Triphone Model (tri1)
- **Type**: Context-dependent triphones
- **States**: 456 PDFs
- **Gaussians**: 10,039 (~22 per state)
- **Context Width**: 3 (left + center + right phone)
- **Features**: 39-D (MFCC + Î” + Î”Î”)
- **Purpose**: Context modeling

#### 3. LDA+MLLT Model (tri2b)
- **Type**: Feature-transformed triphone
- **States**: 528 PDFs
- **Gaussians**: 13,867 (~26 per state)
- **Context Width**: 3
- **Feature Dimension**: 40-D (after LDA transform)
- **Purpose**: Feature transformation and dimensionality reduction

### Training Algorithm: Baum-Welch (EM)

```
1. Initialization: Set initial HMM parameters (Ï€, A, B)
2. E-Step (Expectation):
   - Use Forward-Backward algorithm
   - Compute state occupation probabilities Î³_t(i)
   - Compute state transition probabilities Î¾_t(i,j)
3. M-Step (Maximization):
   - Update transition probabilities: A = f(Î¾)
   - Update observation probabilities: B = f(Î³)
   - Update GMM parameters: Î¼, Î£, w = f(Î³)
4. Iterate until convergence (log-likelihood change < threshold)
```

### Decoding Algorithm: Viterbi

```
Input: Observation sequence O = {o_1, o_2, ..., o_T}
Output: Best state sequence Q* = {q_1*, q_2*, ..., q_T*}

1. Initialization:
   Î´_1(i) = Ï€_i Â· b_i(o_1)
   Ïˆ_1(i) = 0

2. Recursion (t = 2 to T):
   Î´_t(j) = max_i [Î´_{t-1}(i) Â· a_{ij}] Â· b_j(o_t)
   Ïˆ_t(j) = argmax_i [Î´_{t-1}(i) Â· a_{ij}]

3. Termination:
   P* = max_i [Î´_T(i)]
   q_T* = argmax_i [Î´_T(i)]

4. Backtracking (t = T-1 to 1):
   q_t* = Ïˆ_{t+1}(q_{t+1}*)
```

## ğŸ“ˆ Results

### Model Performance

| Model | States | Gaussians | WER (%) | Training Time | Relative Improvement |
|-------|--------|-----------|---------|---------------|---------------------|
| Monophone (mono) | 112 | 986 | 8.0% | ~30 min | Baseline |
| Triphone (tri1) | 456 | 10,039 | 3.2% | ~2 hours | 60% reduction |
| LDA+MLLT (tri2b) | 528 | 13,867 | **3.0%** | ~3 hours | **62.5% reduction** |

### Performance Metrics

- **Best WER Achieved**: 3.0% (LDA+MLLT model)
- **Accuracy**: 97.0% word-level recognition
- **Feature Dimensions**: 39-D MFCC â†’ 40-D (after LDA transformation)
- **Context Width**: 3 phones (triphone modeling)
- **Beam Width**: Optimized for accuracy-speed tradeoff
- **Total Training Time**: ~5.5 hours for complete pipeline

### Key Observations

1. **Significant Progressive Improvement**: 
   - Monophone â†’ Triphone: 60% WER reduction (8.0% â†’ 3.2%)
   - Triphone â†’ LDA+MLLT: Additional 6.25% improvement (3.2% â†’ 3.0%)
   
2. **Context Modeling Impact**: Triphone models dramatically outperform context-independent monophones

3. **Feature Transformation Benefits**: LDA+MLLT provides optimal feature representation and dimensionality reduction

4. **Low-resource Language Success**: Achieving 97% accuracy on Tamil demonstrates effective adaptation for low-resource languages

5. **Production-Ready Performance**: 3.0% WER is competitive with modern ASR systems for similar datasets

## ğŸ“ Course Mapping

### CO2: Apply Bayesian & Markov Networks
- **HMM**: Transition probability modeling (Markov Network)
- **GMM**: Observation probability modeling (Bayesian mixture)

### CO3: Apply Probabilistic Reasoning for Decision Making
- **Observation Probability**: GMM gradient as input for HMM
- **Transition Probability**: Modeled with HMM for sequential reasoning
- **Lexicon Integration**: Enables complex decision-making in ASR

### CO4: Apply Computational Tools for Probabilistic Models
- **EM Algorithm**: Training GMM parameters
- **Baum-Welch Algorithm**: Training HMM parameters
- **ASR Pipeline**: Combines GMM+HMM+Lexicon with modern frameworks

## ğŸ‘¥ Contributors

**Group 5 - Probabilistic Reasoning (22AIE301)**

- **Antonio Roger** (CB.SC.U4AIE23104)
- **Adarsh Pradeep** (CB.SC.U4AIE23109)
- **Mohan Raj** (CB.SC.U4AIE23147)
- **Naresh Kumar V** (CB.SC.U4AIE23165)

## ğŸ“š References

1. Farheen Fauziya & Geeta Nijhawan. "A Comparative Study of Phoneme Recognition using GMM-HMM and ANN based Acoustic Modeling." *International Journal of Computer Applications*, Volume 98, No. 6, July 2014.

2. Abdelmadjid Benmachiche & Amina Makhlouf. "Optimization of Hidden Markov Model With Gaussian Mixture Densities." *WSEAS Transactions on Signal Processing*, Volume 15, 2019.

3. Yi Liu, Liang He, Weiqiang Zhang, Jia Liu, Michael T. Johnson. "Investigation of Frame Alignments for GMM-based Digit-prompted Speaker Verification." arXiv:1710.10436, 2017.

4. Edmondo Trentin & Marco Gori. "Robust Combination of Neural Networks and Hidden Markov Models for Speech Recognition." *IEEE Transactions on Neural Networks*, Volume 14, Issue 6, November 2003.

5. Liang Lu. "Subspace Gaussian Mixture Models for Automatic Speech Recognition." PhD Thesis, University of Edinburgh, 2013.

6. Kaldi ASR Documentation: https://kaldi-asr.org/doc/

7. Common Voice Dataset: https://commonvoice.mozilla.org/


## ğŸ™ Acknowledgments

- **Kaldi Speech Recognition Toolkit** for providing the ASR framework
- **Mozilla Common Voice** for the Tamil language dataset
- **Course Instructor** for guidance on probabilistic reasoning concepts
- **SRI International** for the SRILM language modeling toolkit

## ğŸ“§ Contact

For questions or collaboration opportunities, please open an issue or contact the contributors through the university portal.

---

**Note**: This project was developed as part of the academic curriculum for educational purposes. The implementation follows standard ASR practices and leverages established frameworks for reproducibility.
